%! Author = redleader
%! Date = 2021-03-15

@article{eysenbach2021maximum,
abstract = {Many potential applications of reinforcement learning (RL) require guarantees that the agent will perform well in the face of disturbances to the dynamics or reward function. In this paper, we prove theoretically that standard maximum entropy RL is robust to some disturbances in the dynamics and the reward function. While this capability of MaxEnt RL has been observed empirically in prior work, to the best of our knowledge our work provides the first rigorous proof and theoretical characterization of the MaxEnt RL robust set. While a number of prior robust RL algorithms have been designed to handle similar disturbances to the reward function or dynamics, these methods typically require adding additional moving parts and hyperparameters on top of a base RL algorithm. In contrast, our theoretical results suggest that MaxEnt RL by itself is robust to certain disturbances, without requiring any additional modifications. While this does not imply that MaxEnt RL is the best available robust RL method, MaxEnt RL does possess a striking simplicity and appealing formal guarantees.},
archivePrefix = {arXiv},
arxivId = {2103.06257},
author = {Eysenbach, Benjamin and Levine, Sergey},
eprint = {2103.06257},
journal = {arXiv e-prints},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Maximum Entropy RL (Provably) Solves Some Robust RL Problems}},
url = {https://arxiv.org/abs/2103.06257},
year = {2021}
}
@article{Francois-Lavet2018b,
abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decisionmaking tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
archivePrefix = {arXiv},
arxivId = {1811.12560},
author = {Fran{\c{c}}ois-Lavet, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
doi = {10.1561/2200000071},
eprint = {1811.12560},
issn = {19358245},
journal = {Foundations and Trends in Machine Learning},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{An introduction to deep reinforcement learning}},
year = {2018}
}
@inproceedings{Haarnoja2018g,
abstract = {Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.},
archivePrefix = {arXiv},
arxivId = {1803.06773},
author = {Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey},
booktitle = {Proceedings - IEEE International Conference on Robotics and Automation},
doi = {10.1109/ICRA.2018.8460756},
eprint = {1803.06773},
isbn = {9781538630815},
issn = {10504729},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Composable Deep Reinforcement Learning for Robotic Manipulation}},
year = {2018}
}
@inproceedings{Haarnoja2017b,
abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actor-critic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
archivePrefix = {arXiv},
arxivId = {1702.08165},
author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
booktitle = {34th International Conference on Machine Learning, ICML 2017},
eprint = {1702.08165},
isbn = {9781510855144},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Reinforcement learning with deep energy-based policies}},
year = {2017}
}
@inproceedings{Haarnoja2018f,
abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy-that is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actorcritic formulation, our method achieves state-ofthe-art performance on a range of continuous control benchmark tasks, outperforming prior onpolicy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
archivePrefix = {arXiv},
arxivId = {1801.01290},
author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
booktitle = {35th International Conference on Machine Learning, ICML 2018},
eprint = {1801.01290},
isbn = {9781510867963},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor}},
year = {2018}
}
@misc{Levine2018a,
abstract = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
archivePrefix = {arXiv},
arxivId = {1805.00909},
author = {Levine, Sergey},
booktitle = {arXiv},
eprint = {1805.00909},
issn = {23318422},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}},
year = {2018}
}
@inproceedings{Schulman2015c,
abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1502.05477},
author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael and Abbeel, Pieter},
booktitle = {32nd International Conference on Machine Learning, ICML 2015},
eprint = {1502.05477},
isbn = {9781510810587},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Trust region policy optimization}},
year = {2015}
}
@article{Sutton1998,
abstract = {CiteSeerX - Scientific documents that cite the following paper: Reinforcement learning: An introduction, chapter 11},
author = {Sutton, R.S. and Barto, A.G.},
doi = {10.1109/tnn.1998.712192},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Reinforcement Learning: An Introduction}},
year = {1998}
}
@article{Tuomas2018,
abstract = {In this thesis, we study how maximum entropy framework can provide efficient deep rein- forcement learning (deep RL) algorithms that solve tasks consistently and sample efficiently. This framework has several intriguing properties. First, the optimal policies are stochastic, improving exploration and preventing convergence to local optima, particularly when the objective is multimodal. Second, the entropy term provides regularization, resulting in more consistent and robust learning when compared to deterministic methods. Third, maximum entropy policies are composable, that is, two or more policies can be combined, and the resulting policy can be shown to be approximately optimal for the sum of the constituent task rewards. And fourth, the view of maximum entropy RL as probabilistic inference pro- vides a foundation for building hierarchical policies that can solve complex and sparse reward tasks. In the first part, we will devise new algorithms based on this framework, starting from soft Q-learning that learns expressive energy-based policies, to soft actor-critic that provides simplicity and convenience of actor-critic methods, and ending with automatic temperature adjustment scheme that practically eliminates the need for hyperparameter tuning, which is a crucial feature for real-world applications where tuning of hyperparameters can be pro- hibitively expensive. In the second part, we will discuss extensions enabled by the inherent stochasticity of maximum entropy polices, including compositionality and hierarchical learn- ing. We will demonstrate the effectiveness of the proposed algorithms on both simulated and real-world robotic manipulation and locomotion tasks.},
author = {Tuomas, Haarnoja and Levine, Sergey},
journal = {Electrical Engineering and Computer Sciences University of California at Berkeley},
keywords = {deep reinforcement learning,robotics},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning}},
year = {2018}
}
@inproceedings{Zhao2020a,
abstract = {{Deep{\}} reinforcement learning has recently seen huge success across multiple areas in the robotics domain. Owing to the limitations of gathering real-world data, i.e., sample inefficiency and the cost of collecting it, simulation environments are utilized for training the different agents. This not only aids in providing a potentially infinite data source, but also alleviates safety concerns with real robots. Nonetheless, the gap between the simulated and real worlds degrades the performance of the policies once the models are transferred into real robots. Multiple research efforts are therefore now being directed towards closing this sim-toreal gap and accomplish more efficient policy transfer. Recent years have seen the emergence of multiple methods applicable to different domains, but there is a lack, to the best of our knowledge, of a comprehensive review summarizing and putting into context the different methods. In this survey paper, we cover the fundamental background behind sim-to-real transfer in deep reinforcement learning and overview the main methods being utilized at the moment: domain randomization, domain adaptation, imitation learning, meta-learning and knowledge distillation. We categorize some of the most relevant recent works, and outline the main application scenarios. Finally, we discuss the main opportunities and challenges of the different approaches and point to the most promising directions.},
archivePrefix = {arXiv},
arxivId = {2009.13303},
author = {Zhao, Wenshuai and Queralta, Jorge Pena and Westerlund, Tomi},
booktitle = {2020 IEEE Symposium Series on Computational Intelligence, SSCI 2020},
doi = {10.1109/SSCI47803.2020.9308468},
eprint = {2009.13303},
isbn = {9781728125473},
keywords = {Deep Reinforcement Learning,Domain Randomization,Imitation Learning,Knowledge Distillation,Meta Learning,Robotics,Simto-Real,Transfer Learning},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Sim-to-Real Transfer in Deep Reinforcement Learning for Robotics: A Survey}},
year = {2020}
}
@article{Doyle1978,
abstract = {There are none. Copyright {\textcopyright} 1978 by The Institute of Electrical and Electronics Engineers, Inc.},
author = {Doyle, John C.},
doi = {10.1109/TAC.1978.1101812},
issn = {15582523},
journal = {IEEE Transactions on Automatic Control},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Guaranteed Margins for LQG Regulators}},
year = {1978}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses 'value networks' to evaluate board positions and 'policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {Van Den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Lecture dirig{\'{e}} - lit{\'{e}}rature/depth knowledge/Fondation/Model-based /Dynamique connue/Expert iteration (Monte Carlo Tree search)/Mastering the game of Go with deep neural networks and tree search.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7587},
pages = {484--489},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://dx.doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@misc{Silver2018,
abstract = {{\textless}p{\textgreater}The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.{\textless}/p{\textgreater}},
author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
booktitle = {Science},
doi = {10.1126/science.aar6404},
file = {:Users/redleader/Desktop/Projet de lecture dirig{\'{e}} 2019 - Luc Coupal/Travail Pratique/Applied DRL/Game (optional)/alphazero{\_}preprint.pdf:pdf},
issn = {10959203},
number = {6419},
pages = {1140--1144},
title = {{A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play}},
url = {https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf},
volume = {362},
year = {2018}
}
@article{DeepMind2019,
abstract = {Games have been used for decades as an important way to test and evaluate the performance of artificial intelligence systems. As capabilities have increased, the research community has sought games with increasing complexity that capture different elements of intelligence required to solve scientific and real-world problems. In recent years, StarCraft, considered to be one of the most challenging Real-Time Strategy (RTS) games and one of the longest-played esports of all time, has emerged by consensus as a “grand challenge” for AI research.},
author = {DeepMind, The-AlphaStar-Team},
journal = {DeepMind},
keywords = {AlphaStar},
mendeley-tags = {AlphaStar},
title = {{AlphaStar: Mastering the Real-Time Strategy Game StarCraft II}},
url = {https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/},
year = {2019}
}
@article{Mnih2015a,
abstract = {The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
file = {:Users/redleader/Library/Application Support/Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7540},
pages = {529--533},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf},
volume = {518},
year = {2015}
}
@inproceedings{Inoue2017,
abstract = {The high precision assembly of mechanical parts requires precision that exceeds that of robots. Conventional part-mating methods used in the current manufacturing require numerous parameters to be tediously tuned before deployment. We show how a robot can successfully perform a peg-in-hole task with a tight clearance through training a recurrent neural network with reinforcement learning. In addition to reducing manual effort, the proposed method also shows a better fitting performance with a tighter clearance and robustness against positional and angular errors for the peg-in-hole task. The neural network learns to take the optimal action by observing the sensors of a robot to estimate the system state. The advantages of our proposed method are validated experimentally on a 7-axis articulated robot arm.},
archivePrefix = {arXiv},
arxivId = {1708.04033},
author = {Inoue, Tadanobu and {De Magistris}, Giovanni and Munawar, Asim and Yokoya, Tsuyoshi and Tachibana, Ryuki},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS.2017.8202244},
eprint = {1708.04033},
isbn = {9781538626825},
issn = {21530866},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Deep reinforcement learning for high precision assembly tasks}},
year = {2017}
}
@article{Morimoto2005,
abstract = {This letter proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors. The use of environmental models in RL is quite popular for both off-line learning using simulations and for online action planning. However, the difference between the model and the real environment can lead to unpredictable, and often unwanted, results. Based on the theory of H∞ control, we consider a differential game in which a "disturbing" agent tries to make the worst possible disturbance while a "control" agent tries to make the best control input. The problem is formulated as finding a min-max solution of a value function that takes into account the amount of the reward and the norm of the disturbance. We derive online learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function. We tested the paradigm, which we call robust reinforcement learning (RRL), on the control task of an inverted pendulum. In the linear domain, the policy and the value function learned by online algorithms coincided with those derived analytically by the linear H ∞ control theory. For a fully nonlinear swing-up task, RRL achieved robust performance with changes in the pendulum weight and friction, while a standard reinforcement learning algorithm could not deal with these changes. We also applied RRL to the cart-pole swing-up task, and a robust swing-up policy was acquired.},
author = {Morimoto, Jun and Doya, Kenji},
doi = {10.1162/0899766053011528},
issn = {08997667},
journal = {Neural Computation},
pmid = {15720771},
title = {{Robust reinforcement learning}},
year = {2005}
}
@misc{Donti2020,
abstract = {When designing controllers for safety-critical systems, practitioners often face a challenging tradeoff between robustness and performance. While robust control methods provide rigorous guarantees on system stability under certain worst-case disturbances, they often result in simple controllers that perform poorly in the average (non-worst) case. In contrast, nonlinear control methods trained using deep learning have achieved state-of-the-art performance on many control tasks, but often lack robustness guarantees. In this paper, we propose a technique that combines the strengths of these two approaches: a generic nonlinear control policy class, parameterized by neural networks, that nonetheless enforces the same provable robustness criteria as robust control. Specifically, we show that by integrating custom convex-optimization-based projection layers into a nonlinear policy, we can construct a provably robust neural network policy class that outperforms robust control methods in the average (non-adversarial) setting. We demonstrate the power of this approach on several domains, improving in performance over existing robust control methods and in stability over (non-robust) RL methods.},
archivePrefix = {arXiv},
arxivId = {2011.08105},
author = {Donti, Priya L. and Fazlyab, Mahyar and Roderick, Melrose and {Zico Kolter}, J.},
booktitle = {arXiv},
eprint = {2011.08105},
issn = {23318422},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Enforcing robust control guarantees within neural network policies}},
year = {2020}
}
@article{Tuomas2018a,
abstract = {In this thesis, we study how maximum entropy framework can provide efficient deep rein- forcement learning (deep RL) algorithms that solve tasks consistently and sample efficiently. This framework has several intriguing properties. First, the optimal policies are stochastic, improving exploration and preventing convergence to local optima, particularly when the objective is multimodal. Second, the entropy term provides regularization, resulting in more consistent and robust learning when compared to deterministic methods. Third, maximum entropy policies are composable, that is, two or more policies can be combined, and the resulting policy can be shown to be approximately optimal for the sum of the constituent task rewards. And fourth, the view of maximum entropy RL as probabilistic inference pro- vides a foundation for building hierarchical policies that can solve complex and sparse reward tasks. In the first part, we will devise new algorithms based on this framework, starting from soft Q-learning that learns expressive energy-based policies, to soft actor-critic that provides simplicity and convenience of actor-critic methods, and ending with automatic temperature adjustment scheme that practically eliminates the need for hyperparameter tuning, which is a crucial feature for real-world applications where tuning of hyperparameters can be pro- hibitively expensive. In the second part, we will discuss extensions enabled by the inherent stochasticity of maximum entropy polices, including compositionality and hierarchical learn- ing. We will demonstrate the effectiveness of the proposed algorithms on both simulated and real-world robotic manipulation and locomotion tasks.},
author = {Tuomas, Haarnoja and Levine, Sergey},
journal = {Electrical Engineering and Computer Sciences University of California at Berkeley},
keywords = {deep reinforcement learning,robotics},
mendeley-groups = {IFT-6001 Synth{\`{e}}se},
title = {{Acquiring Diverse Robot Skills via Maximum Entropy Deep Reinforcement Learning}},
year = {2018}
}
